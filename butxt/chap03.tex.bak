\chapter{Neurónové siete}

\iffalse
Formálne  je neurónová    sieť    určená    ako    orientovaný    graf    G=(V,E).    
Výrazy V={v1,v2,...,vN}  a  E={e1,e2,...eM}  označujú  neprázdnu  vrcholovú  množinu  resp.  hranovú množinu grafu G obsahujúceho N vrcholov (neurónov) a M hrán (spojov). Každý spoj $e \in E$ sa interpretuje ako usporiadaná dvojica dvoch neurónov z množiny V, e=(v,v’).  
Hovoríme, že  spoj  e  začína  v  neuróne  v  a  končí  v  neuróne  v’.  Množina  neurónov  V  je  rozložená  na disjunktné podmnožiny.

kde VI obsahuje NI vstupných neurónov, ktoré sú susedné len s vychádzajúcimi hranami, VH obsahuje NH skrytých (angl. hidden) neurónov, ktoré sú susedné súčasne  s  vychádzajúcimi ako aj s vchádzajúcimi hranami, a konečne VO obsahuje NO výstupných neurónov, ktoré súsusedné  len  s  vchádzajúcimi  hranami.  
V  našich  nasledujúcich  úvahách  budeme  vždy predpokladať,  že  množiny  VI  a  VO  sú  neprázdne,  t.j.  neurónová  sieť  obsahuje  vždy  aspoň jeden vstupný a jeden výstupný neurón.
Pre acyklické neurónové siete (ktoré neobsahujú orientované cykly neuróny môžu byť usporiadané do vrstiev VL  L  $LL=\cup \cup \cup \cup 12 3...t$, kde L1=VI je vstupná vrstva (obsahuje len vstupné neuróny), L2, L3,..., Lt–1 sú skryté vrstvya  Lt  je  výstupná  vrstva.  
Vrstva  Li  (pre  $1\leq i\leq t$)  je  určená  nasledujúcim  jednoduchým spôsobom $LVivdvi=\in   =+;aflq1$, kde vzdialenosť d(v) sa rovná dĺžke maximálnej cesty, ktorá spája daný neurón so vstupným neurónom, potom musí platiť d(v)=0, pre $v\in VI$. 
Neurónová sieť určená acyklickým grafom je  obvykle  volená  tak,  že  neuróny  z  dvoch  susedných  vrstiev  sú  poprepájané  všetkými možnými  spojmi.  
Žiaľ,  takýto  rozklad  množiny  neurónov  na  vrstvy  je Obrázok 5.2. 
Neurónová sieť je definovaná ako orientovaný súvislý graf. 
Diagram A obsahuje orientovaný graf  s  jedným  cyklom  a  teda  nemôže  byť  použitý  pre  definíciu  neurónovej  siete  s  dopredným  šírením.
Diagram  B  ilustruje  možnosť  rozkladu  vrcholov  (neurónov)  acyklického  orientovaného  grafu  na  vrstvy L1,..,L4.
kde L1=VI je vstupná vrstva (obsahuje len vstupné neuróny), L2, L3,..., Lt–1 sú skryté vrstvya  Lt  je  výstupná  vrstva.  Vrstva  Li  (pre  $1\leq i\leq t$)  je  určená  nasledujúcim  jednoduchýmspôsobom$LVivdvi=\in   =+$;aflq1                                  (5.9)kde vzdialenosťd(v) sa rovná dĺžke maximálnej cesty, ktorá spája daný neurón so vstupnýmneurónom, potom musí platiťd(v)=0, pre $v\in VI$. Neurónová sieť určená acyklickým grafomje  obvykle  volená  tak,  že  neuróny  z  dvoch  susedných  vrstiev  sú  poprepájané  všetkýmimožnými  spojmi  (pozri  obr.  5.3). \citep{rnn:spol}

\fi

%\iffalse
Neural networks are composed of nodes or units (see Figure 18.19) connected by directed links. 
A link from unit i to unit j serves to propagate the activation $a_i$ from i to j.
Each link also has a numeric weight $w_{i,j}$ associated with it, which determines the strength and sign of the connection. 
Just as in linear regression models, each unit has a dummy input $a_0=1$ with an associated weight $w_{0,j}$. 
Each unit j first computes a weighted sum of its inputs: $$in_j=\sum^n_{i=0}w_{i,j}a_i$$
Then it applies anactivation function g to this sum to derive the output: 
$$a_j=g(in_j)=g\left(\sum^n_{i=0}w_{i,j}a_i\right)$$

The activation function $g$ is typically either a hard threshold (Figure 18.17(a)), in which case the unit is called a perceptron, or a logistic function (Figure 18.17(b)), in which case the term sigmoid perceptron is sometimes used.  Both of these nonlinear activation function ensure the important property that the entire network of units can represent a nonlinear function (seeExercise 18.22). As mentioned in the discussion of logistic regression (page 725), the logistic activation function has the added advantage of being differentiable. Having decided on the mathematical model for individual “neurons,”  the next task is to connect them together to form a network.  There are two fundamentally distinct ways to do this.  

A feed-forward network has connections only in one direction—that is, it forms a directed acyclic graph. Every node receives input from “upstream” nodes and delivers output to “downstream” nodes; there are no loops. A feed-forward network represents a function of its current input; thus, it has no internal state other than the weights themselves. 

A recurrent network,  on  the  other  hand,  feeds  its  outputs  back  into  its  own  inputs.   This  means  that the activation levels of the network form a dynamical system that may reach a stable state or exhibit oscillations or even chaotic behavior. Moreover, the response of the network to a given input depends on its initial  state,  which may depend on previous inputs.  Hence,  recurrent networks (unlike feed-forward networks) can support short-term memory.  This makes them more interesting as models of the brain, but also more difficult to understand.

Feed-forward networks are usually arranged in layers, such that each unit receives input only from units in the immediately preceding layer. In the next two subsections, we will look at single-layer networks, in which every unit connects directly from the network’s inputs to its outputs, and multilayer networks, which have one or more layers of hidden units that are not connected to the outputs of the network. 


%\fi

\section{Popredné neurónové siete}

\section{Rekurentné neurónové siete}
Vo  všeobecnosti  možno  za  rekurentnú  sieť  považovať akúkoľvek  neurónovú  sieť,  v  ktorej  istá  podmnožina  neurónov  (rekurentné  neuróny)  jes chopná  uchovať  informáciu  o  svojich  aktiváciách  v  predošlých  časoch  pre  výpočet aktivácií  neurónov  v  čase t+1.  “Odpamätané”  hodnoty  sa  objavia  v  čase t+1  ako  aktivácie tzv. kontextových   neurónov \citep{rnn:spol}